import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
import joblib
import os

print("=" * 80)
print("CARDIONET - Entrenamiento de Modelos de PredicciÃ³n de CardiopatÃ­as")
print("=" * 80)

# Cargar dataset
print("\n[1] Cargando dataset...")
data_path = os.path.join('..', 'data', 'heart_disease_sample.csv')

if not os.path.exists(data_path):
    print(f"Error: No se encontrÃ³ el archivo {data_path}")
    print("Por favor, asegÃºrate de tener el dataset 'heart.csv' en la carpeta 'data'")
    exit(1)

df = pd.read_csv(data_path)
print(f"âœ“ Dataset cargado exitosamente: {df.shape[0]} filas, {df.shape[1]} columnas")

# ExploraciÃ³n bÃ¡sica
print("\n[2] Resumen del dataset:")
print(df.head(10))
print("\nInformaciÃ³n del dataset:")
print(df.info())
print("\nEstadÃ­sticas descriptivas:")
print(df.describe())

# Verificar valores nulos
print("\n[3] Verificando valores nulos:")
nulls = df.isnull().sum()
if nulls.sum() == 0:
    print("âœ“ No se encontraron valores nulos en el dataset")
else:
    print(nulls)

# Separar caracterÃ­sticas y variable objetivo
print("\n[4] Preparando datos para entrenamiento...")
X = df.drop('target', axis=1)
y = df['target']

print(f"CaracterÃ­sticas (X): {X.shape}")
print(f"Variable objetivo (y): {y.shape}")
print(f"\nDistribuciÃ³n de clases:")
print(f"  - Sin enfermedad cardÃ­aca (0): {(y == 0).sum()} pacientes ({(y == 0).sum() / len(y) * 100:.2f}%)")
print(f"  - Con enfermedad cardÃ­aca (1): {(y == 1).sum()} pacientes ({(y == 1).sum() / len(y) * 100:.2f}%)")

# DivisiÃ³n del dataset en 80% entrenamiento y 20% prueba
print("\n[5] Dividiendo dataset en 80% entrenamiento y 20% prueba...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1, stratify=y
)

print(f"âœ“ Conjunto de entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0] / len(X) * 100:.1f}%)")
print(f"âœ“ Conjunto de prueba: {X_test.shape[0]} muestras ({X_test.shape[0] / len(X) * 100:.1f}%)")

# No usar StandardScaler para mantener consistencia con MATLAB
print("\n[6] Preparando datos (sin normalizaciÃ³n para consistencia con modelo MATLAB)...")
X_train_data = X_train.values
X_test_data = X_test.values
y_train_data = y_train.values
y_test_data = y_test.values

# ============================================
# MODELO: BOSQUE ALEATORIO (RANDOM FOREST)
# Equivalente al TreeBagger de MATLAB
# ============================================
print("\n" + "=" * 80)
print("ENTRENAMIENTO DEL BOSQUE ALEATORIO (RANDOM FOREST)")
print("=" * 80)

print("\n[7] Configurando modelo Random Forest (equivalente a TreeBagger)...")
print("    - NÃºmero de Ã¡rboles: 100")
print("    - MÃ©todo: ClasificaciÃ³n")
print("    - OOB Prediction: Activado")
print("    - Semilla aleatoria: 1 (para reproducibilidad)")

# Entrenamiento del Bosque Aleatorio
# Equivalente a: TreeBagger(100, X_train, Y_train, 'Method', 'classification',
#                           'OOBPrediction', 'on', 'OOBPredictorImportance', 'on')
rf_model = RandomForestClassifier(
    n_estimators=100,          # NÃºmero de Ã¡rboles en el bosque
    criterion='gini',          # FunciÃ³n de divisiÃ³n
    max_features='sqrt',       # MÃ¡ximo de caracterÃ­sticas por divisiÃ³n
    bootstrap=True,            # Bootstrap sampling
    oob_score=True,            # Calcular OOB score
    random_state=1,            # Semilla para reproducibilidad (rng(1) en MATLAB)
    n_jobs=-1,                 # Usar todos los procesadores
    verbose=0
)

print("\n[8] Entrenando Random Forest...")
rf_model.fit(X_train_data, y_train_data)
print("âœ“ Modelo Random Forest entrenado exitosamente")
print(f"âœ“ OOB Score: {rf_model.oob_score_ * 100:.2f}%")

# Predicciones
print("\n[9] Realizando predicciones en conjunto de prueba...")
y_pred_rf = rf_model.predict(X_test_data)

# CÃ¡lculo de mÃ©tricas (equivalente al cÃ³digo MATLAB)
print("\n[10] Calculando mÃ©tricas de evaluaciÃ³n...")
TP = np.sum((y_test_data == 1) & (y_pred_rf == 1))
TN = np.sum((y_test_data == 0) & (y_pred_rf == 0))
FP = np.sum((y_test_data == 0) & (y_pred_rf == 1))
FN = np.sum((y_test_data == 1) & (y_pred_rf == 0))

accuracy = (TP + TN) / (TP + TN + FP + FN)
precision = TP / (TP + FP) if (TP + FP) > 0 else 0
recall = TP / (TP + FN) if (TP + FN) > 0 else 0
f1score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

# Matriz de confusiÃ³n
cm = confusion_matrix(y_test_data, y_pred_rf)

print("\n" + "=" * 80)
print("ğŸ“Š MÃ‰TRICAS DEL BOSQUE ALEATORIO:")
print("=" * 80)
print(f"Exactitud (Accuracy):       {accuracy * 100:.2f}%")
print(f"PrecisiÃ³n (Precision):      {precision * 100:.2f}%")
print(f"Sensibilidad (Recall):      {recall * 100:.2f}%")
print(f"F1-Score:                   {f1score * 100:.2f}%")

print("\nğŸ“ˆ Matriz de ConfusiÃ³n:")
print(f"    Predicho: 0    Predicho: 1")
print(f"Real: 0    {TN:3d}          {FP:3d}")
print(f"Real: 1    {FN:3d}          {TP:3d}")
print(f"\nVerdaderos Positivos (TP): {TP}")
print(f"Verdaderos Negativos (TN): {TN}")
print(f"Falsos Positivos (FP):     {FP}")
print(f"Falsos Negativos (FN):     {FN}")

# Reporte de clasificaciÃ³n detallado
print("\nğŸ“‹ Reporte de ClasificaciÃ³n Detallado:")
print(classification_report(y_test_data, y_pred_rf, target_names=['Sin enfermedad', 'Con enfermedad']))

# Importancia de las variables (equivalente a OOBPermutedPredictorDeltaError)
print("\n[11] Calculando importancia de variables...")
feature_importance = pd.DataFrame({
    'Variable': X.columns,
    'Importancia': rf_model.feature_importances_
}).sort_values('Importancia', ascending=False)

print("\nğŸ“Š Importancia de las Variables en el Bosque Aleatorio:")
print("=" * 80)
for idx, row in feature_importance.iterrows():
    bar_length = int(row['Importancia'] * 100)
    bar = 'â–ˆ' * bar_length
    print(f"{row['Variable']:25s} â”‚ {bar} {row['Importancia']:.4f}")
print("=" * 80)

# ============================================
# MODELO ADICIONAL: ÃRBOL DE DECISIÃ“N
# ============================================
print("\n" + "=" * 80)
print("ENTRENAMIENTO DE ÃRBOL DE DECISIÃ“N (DECISION TREE)")
print("=" * 80)

print("\n[12] Entrenando Ãrbol de DecisiÃ³n...")
dt_model = DecisionTreeClassifier(
    criterion='gini',
    max_depth=8,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=1
)

dt_model.fit(X_train_data, y_train_data)
print("âœ“ Modelo Decision Tree entrenado exitosamente")

# Predicciones
y_pred_dt = dt_model.predict(X_test_data)

# EvaluaciÃ³n
accuracy_dt = accuracy_score(y_test_data, y_pred_dt)
precision_dt = precision_score(y_test_data, y_pred_dt, zero_division=0)
recall_dt = recall_score(y_test_data, y_pred_dt, zero_division=0)
f1_dt = f1_score(y_test_data, y_pred_dt, zero_division=0)

print("\nğŸ“Š MÃ‰TRICAS DEL ÃRBOL DE DECISIÃ“N:")
print("=" * 80)
print(f"Exactitud (Accuracy):       {accuracy_dt * 100:.2f}%")
print(f"PrecisiÃ³n (Precision):      {precision_dt * 100:.2f}%")
print(f"Sensibilidad (Recall):      {recall_dt * 100:.2f}%")
print(f"F1-Score:                   {f1_dt * 100:.2f}%")

print("\nğŸ“‹ Reporte de ClasificaciÃ³n:")
print(classification_report(y_test_data, y_pred_dt, target_names=['Sin enfermedad', 'Con enfermedad']))

# ============================================
# GUARDAR MODELOS ENTRENADOS
# ============================================
print("\n" + "=" * 80)
print("GUARDANDO MODELOS ENTRENADOS")
print("=" * 80)

models_dir = os.path.join('..', 'ml_models')
if not os.path.exists(models_dir):
    os.makedirs(models_dir)
    print(f"âœ“ Directorio creado: {models_dir}")

print("\n[13] Guardando modelos entrenados...")
joblib.dump(rf_model, os.path.join(models_dir, 'random_forest_model.pkl'))
print(f"âœ“ Random Forest guardado: {os.path.join(models_dir, 'random_forest_model.pkl')}")

joblib.dump(dt_model, os.path.join(models_dir, 'decision_tree_model.pkl'))
print(f"âœ“ Decision Tree guardado: {os.path.join(models_dir, 'decision_tree_model.pkl')}")

# Guardar tambiÃ©n la lista de caracterÃ­sticas para referencia
feature_names = list(X.columns)
joblib.dump(feature_names, os.path.join(models_dir, 'feature_names.pkl'))
print(f"âœ“ Nombres de caracterÃ­sticas guardados: {os.path.join(models_dir, 'feature_names.pkl')}")

# ============================================
# RESUMEN FINAL Y COMPARACIÃ“N
# ============================================
print("\n" + "=" * 80)
print("RESUMEN FINAL - COMPARACIÃ“N DE MODELOS")
print("=" * 80)

print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print("â”‚ MÃ©trica                     â”‚ Random Forest   â”‚ Decision Tree   â”‚")
print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
print(f"â”‚ Exactitud (Accuracy)        â”‚    {accuracy * 100:6.2f}%      â”‚    {accuracy_dt * 100:6.2f}%      â”‚")
print(f"â”‚ PrecisiÃ³n (Precision)       â”‚    {precision * 100:6.2f}%      â”‚    {precision_dt * 100:6.2f}%      â”‚")
print(f"â”‚ Sensibilidad (Recall)       â”‚    {recall * 100:6.2f}%      â”‚    {recall_dt * 100:6.2f}%      â”‚")
print(f"â”‚ F1-Score                    â”‚    {f1score * 100:6.2f}%      â”‚    {f1_dt * 100:6.2f}%      â”‚")
print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

mejor_modelo = 'Random Forest' if accuracy > accuracy_dt else 'Decision Tree'
print(f"\nğŸ† Mejor modelo por Accuracy: {mejor_modelo}")

print("\n" + "=" * 80)
print("âœ“ PROCESO DE ENTRENAMIENTO COMPLETADO EXITOSAMENTE")
print("=" * 80)
print("\nğŸ“Œ Los modelos estÃ¡n listos para ser utilizados en CARDIONET")
print("ğŸ“Œ El modelo Random Forest serÃ¡ usado para las predicciones en producciÃ³n")
print(f"ğŸ“Œ Dataset: {df.shape[0]} pacientes, {df.shape[1] - 1} caracterÃ­sticas")
print(f"ğŸ“Œ OOB Score del Random Forest: {rf_model.oob_score_ * 100:.2f}%")
print("\n" + "=" * 80)
